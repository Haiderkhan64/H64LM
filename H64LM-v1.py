# -*- coding: utf-8 -*-
"""H64LM-v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1po1SsGE0u0AShCaThXtV6xSJXDBTf4px

## üîß Environment Setup

This section imports and configures all dependencies required to train the **H64LM** model from scratch.

- **Standard libraries** ‚Äî file handling, JSON, math, logging, argument parsing, and utilities.  
- **PyTorch core** ‚Äî deep learning framework used for model definition, optimization, and mixed-precision training.  
- **Hugging Face tools** ‚Äî dataset loading and tokenization (`datasets`, `transformers`, `tokenizers`).  
- **Optional acceleration libraries** ‚Äî  
  - `flash_attn` for efficient attention  
  - `vLLM` for inference acceleration  
  - `fastermoe` for Mixture-of-Experts support  
  - `tensorboard` for logging (if available)

Warnings are suppressed for cleaner output, and a simple custom `Logger` is initialized to track runtime information.  
This setup ensures graceful fallback if any optional library is missing.
**bold text**
"""

# Standard library imports
import os
import json
import math
import time
import logging
import hashlib
import argparse
import warnings
from typing import Optional, Tuple, List, Dict, Callable, Any
from datasets import Dataset
from dataclasses import dataclass
from contextlib import contextmanager, nullcontext
from collections import deque

# Third-party core imports
import numpy as np
from tqdm.auto import tqdm
import traceback
import matplotlib.pyplot as plt
import sys

# PyTorch core imports
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
from torch.profiler import profile, record_function, ProfilerActivity
from torch.optim.lr_scheduler import CosineAnnealingLR

# Hugging Face imports
from datasets import load_dataset, Dataset as HFDataset
from transformers import PreTrainedTokenizerFast, AutoTokenizer
from transformers.optimization import get_linear_schedule_with_warmup
from tokenizers import Tokenizer, models, pre_tokenizers, trainers


# Suppress common warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# Disable tokenizers parallelism to avoid warnings
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Optional dependencies with graceful fallback
try:
    from flash_attn import flash_attn_func
    FLASH_ATTN_AVAILABLE = True
except ImportError:
    flash_attn_func = None
    FLASH_ATTN_AVAILABLE = False

try:
    import vllm
    from vllm import LLM, SamplingParams
    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False

try:
    import fastermoe
    from fastermoe import MoELayer
    FASTERMOE_AVAILABLE = True
except ImportError:
    FASTERMOE_AVAILABLE = False

try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    TENSORBOARD_AVAILABLE = False
    SummaryWriter = None


# Custom Logger class
class Logger:
    """Simple logger for training output."""

    def info(self, msg):
        print(f"[INFO] {msg}", flush=True)

    def warning(self, msg):
        print(f"[WARNING] {msg}", flush=True)

    def error(self, msg):
        print(f"[ERROR] {msg}", flush=True)


# Initialize logger
logger = Logger()

# Log availability of optional dependencies
if not FLASH_ATTN_AVAILABLE:
    logger.info("Flash Attention not available, using standard attention")
if not VLLM_AVAILABLE:
    logger.info("vLLM not available")
if not FASTERMOE_AVAILABLE:
    logger.info("FasterMoE not available")
if not TENSORBOARD_AVAILABLE:
    logger.info("TensorBoard not available, logging will be limited")

"""## Logging and Mixed Precision Setup

This section initializes the **logging system** and defines a safe utility for **automatic mixed-precision (AMP)** training.

- The `logging` configuration ensures that all messages (INFO, WARNING, ERROR) are timestamped and printed to the console.  
- The `safe_autocast()` function provides a compatibility-safe wrapper for mixed-precision context management across different PyTorch versions.  
  - Uses `torch.cuda.amp.autocast` when CUDA is available.  
  - Falls back to a `nullcontext()` on CPU or unsupported environments.  

This setup provides consistent logging and safe mixed-precision usage across GPUs and CPUs.

"""

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def safe_autocast(device, dtype=torch.float16):
    """Safe autocast wrapper that handles different PyTorch versions."""
    if device.type == "cuda" and torch.cuda.is_available():
        try:
            return torch.cuda.amp.autocast(dtype=dtype)
        except TypeError:
            return torch.cuda.amp.autocast(enabled=True, dtype=dtype)
    return nullcontext()

"""## Model and Training Configuration Classes

This section defines two key configuration dataclasses that store hyperparameters and runtime options for both **training** and **model architecture**.

### `H64LMTrainingConfig`
Contains training-related options:
- **use_fp16:** Enables mixed-precision (FP16) training for better performance on GPUs.  
- **gradient_checkpointing:** Saves memory by re-computing intermediate activations during backward pass.  
- **use_cache:** Controls whether model caching (e.g., key/value states) is enabled during training.  

### `H64LMConfig`
Defines the complete model architecture and runtime behavior:
- **Model Architecture:** Parameters like `hidden_size`, `num_layers`, `num_attention_heads`, and `max_position_embeddings` define the model‚Äôs structure.  
- **Mixture of Experts (MoE):** Options such as `num_experts`, `expert_hidden_size`, `capacity_factor`, and `load_balance_loss_coeff` configure the expert routing system.  
- **Attention:** Includes flexibility for flash attention, RoPE embeddings, and sliding-window attention mechanisms.  
- **Optimization & Regularization:** Parameters like `dropout`, `z_loss_coeff`, and `diversity_loss_coeff` help with stability and generalization.  
- **Training Parameters:** Includes batch size, epochs, checkpoint intervals, and tokenizer settings.  
- **Paths & Runtime:** Specifies dataset directories, checkpoint loading, and distributed (DDP) or streaming support.

The `__post_init__()` method ensures derived parameters (like `head_dim`) are computed automatically based on other settings.

"""

@dataclass
class H64LMTrainingConfig:
    use_fp16: bool = True
    gradient_checkpointing: bool = True and torch.cuda.is_available()
    use_cache: bool = True



@dataclass
class H64LMConfig:
    vocab_size: int = 32000
    hidden_size: int = 768
    num_layers: int = 6
    num_attention_heads: int = 12
    num_kv_heads: int = 4
    max_position_embeddings: int = 1024
    num_experts: int = 8
    num_experts_per_token: int = 2
    expert_hidden_size: int = hidden_size * 4
    use_flash_attention: bool = False
    kv_block_size: int = 256
    layer_norm_eps: float = 1e-5
    dropout: float = 0.1
    expert_dropout: float = 0.05
    capacity_factor: float = 1.25
    load_balance_loss_coeff: float = 0.01
    pad_token_id: int = 0
    bos_token_id: int = 1
    eos_token_id: int = 2
    rope_theta: float = 10000.0
    initializer_range: float = 0.02
    tie_word_embeddings: bool = False
    moe_temperature: float = 2.0
    diversity_loss_coeff: float = 0.002
    z_loss_coeff: float = 1e-3
    initializer_type: str = "scaled"
    attention_type: str = "rope"
    sliding_window_size: int = 2048
    use_attention_sinks: bool = True
    kv_cache_fp16: bool = False
    gradient_checkpointing: bool = True
    num_epochs=1
    batch_size=16
    grad_accum_steps=8
    val_interval=None
    save_interval=500000
    log_interval=10
    use_fp16=True,
    use_tensorboard=False
    checkpoint_dir="checkpoints"
    resume_from = "checkpoints_h64lm/best_model_state_dict.pt"
    use_ddp=True
    use_pretrained_tokenizer="mistral_tokenizer"
    streaming=False
    use_cache: bool = True
    dataset="parquet"
    dataset_dir="data/wikitext-103/wikitext-103-raw-v1"

    max_samples: int = 100

    def __post_init__(self):
        self.head_dim = self.hidden_size // self.num_attention_heads
        if self.attention_type == "mqa":
            self.num_kv_heads = 1

"""## RMSNorm: Root Mean Square Layer Normalization

This class implements **RMSNorm**, a lightweight normalization technique used in many modern LLMs (e.g., GPT-NeoX, LLaMA).

### Key Details
- **Purpose:** Normalizes activations based on their root mean square (RMS) rather than mean and variance (as in LayerNorm).  
- **Computation:**  
  1. Converts input to `float32` for numerical stability.  
  2. Computes the mean of squared values across the last dimension.  
  3. Scales the input by the inverse RMS, controlled by a small epsilon to avoid division by zero.  
  4. Multiplies by a learned scaling parameter (`self.weight`).  

### Parameters
- `hidden_size`: Dimensionality of the input embeddings.  
- `eps`: Small constant for numerical stability (default: `1e-5`).  

### Advantages
- More computationally efficient than LayerNorm.  
- Removes the need to compute the mean, making it slightly faster and more stable in half-precision (FP16) training.  
- Commonly used in large transformer models.


"""

class RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-5):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return (self.weight.to(input_dtype) * hidden_states.to(input_dtype))

"""## Weight Initialization Function

The `initialize_weights()` function defines a **custom, depth-aware weight initialization** strategy for all model components.  
It ensures stable training, especially in deep transformer-based architectures like H64LM.

### Key Functionality
- **Depth-aware scaling:**  
  Uses a scaling factor based on layer depth (`‚àö(2 / (depth + 1))`) to gradually reduce initialization variance in deeper layers.  
- **Module-specific initialization:**
  - **Linear layers:** Weights are drawn from a normal distribution with standard deviation adjusted by `fan_in` and `depth_scale`. Biases are initialized to zero.  
  - **Embedding layers:** Similar to linear layers but use `hidden_size` as `fan_in`.  
  - **Normalization layers (LayerNorm, RMSNorm):** Weight set to ones; bias (if exists) set to zero.  

### Parameters
- `module`: The PyTorch module (layer) to initialize.  
- `config`: The model configuration object (`H64LMConfig`) that provides initialization constants such as `initializer_range`.  
- `depth`: Optional layer depth to scale variance during initialization (helps deeper networks maintain stability).

### Why This Matters
Proper weight initialization:
- Prevents vanishing or exploding gradients.  
- Encourages stable convergence during training.  
- Reduces the need for aggressive learning rate tuning.  
- Works effectively with FP16 training and MoE architectures.


"""

def initialize_weights(module, config: H64LMConfig, depth: int = 0):
    """Initialize weights with depth-aware scaling."""
    depth_scale = math.sqrt(2.0 / max(1.0, (depth + 1)))
    if isinstance(module, nn.Linear):
        if hasattr(module, 'weight') and module.weight is not None:
            if module.weight.dim() >= 2:
                fan_in = module.weight.shape[1]
            else:
                fan_in = module.weight.numel()
            std = config.initializer_range * depth_scale / math.sqrt(fan_in)
            nn.init.normal_(module.weight, mean=0.0, std=std)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        fan_in = config.hidden_size
        std = config.initializer_range / math.sqrt(fan_in)
        nn.init.normal_(module.weight, mean=0.0, std=std)
    elif isinstance(module, (nn.LayerNorm, RMSNorm)):
        nn.init.ones_(module.weight)
        if hasattr(module, 'bias') and module.bias is not None:
            nn.init.zeros_(module.bias)

"""## Rotary Positional Embeddings (RoPE)

This section implements **Rotary Position Embeddings (RoPE)** ‚Äî a modern, efficient method for encoding token position information directly within the attention mechanism.  
RoPE is widely used in advanced LLMs such as **GPT-NeoX**, **LLaMA**, and **Mistral**.

---

### `RotaryEmbedding` Class
Generates the sine and cosine frequency matrices required for RoPE.

#### Key Details:
- **Inputs:**
  - `dim`: Dimensionality of the embedding vectors.
  - `max_position_embeddings`: Maximum supported sequence length.
  - `base`: Frequency scaling base (default: `10000`).
- **Computation:**
  - Calculates inverse frequency (`inv_freq`) values based on embedding dimension.
  - Generates precomputed sine (`sin`) and cosine (`cos`) tables for use in attention layers.
- **Implementation Note:**
  - Uses `register_buffer(..., persistent=True)` to ensure buffers are included in the model‚Äôs state dictionary during saving and loading.

---

### `apply_rotary_pos_emb` Function
Applies the precomputed RoPE embeddings to **query (Q)** and **key (K)** tensors.

#### Process:
1. Computes position indices for each token in the sequence.
2. Validates that position IDs are within bounds of the maximum allowed positions.
3. Combines input embeddings with sinusoidal rotations (`cos` and `sin`) using the helper function `rotate_half()`.

#### Advantages:
- Enables smooth position representation in continuous space.
- Maintains **rotational invariance** ‚Äî enhancing generalization beyond training sequence lengths.
- Requires **no additional trainable parameters**, keeping the model lightweight.

---

### `rotate_half(x)` Helper
Splits an input tensor into two halves and rotates them to apply the positional transformation:
\[
\text{RoPE}(x) = [ -x_2, \; x_1 ]
\]
This is the core operation that introduces the **rotational effect** in embedding space.

---

### Summary
| Component | Purpose | Output |
|------------|----------|---------|
| `RotaryEmbedding` | Precompute sine & cosine frequencies | `(cos, sin)` tensors |
| `apply_rotary_pos_emb` | Apply rotation to attention tensors | Rotated `(Q, K)` |
| `rotate_half` | Core vector rotation utility | Transformed tensor |

RoPE effectively replaces absolute positional embeddings with a **context-aware rotation**, improving extrapolation and efficiency in large sequence modeling.

"""

class RotaryEmbedding(nn.Module):
    def __init__(self, dim, max_position_embeddings=1000000, base=10000):
        super().__init__()
        # FIX #3: Using persistent=True to save buffer in state_dict
        self.dim = dim
        self.max_position_embeddings = max_position_embeddings
        self.base = base
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer("inv_freq", inv_freq, persistent=True)

    def forward(self, device, seq_len):
        t = torch.arange(seq_len, device=device, dtype=self.inv_freq.dtype)
        freqs = torch.outer(t, self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        return emb.cos(), emb.sin()

def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None):
    """Apply rotary position embeddings with proper bounds checking."""
    batch_size, _, seq_len, _ = q.size()
    device = q.device
    if position_ids is None:
        position_ids = torch.arange(seq_len, device=device, dtype=torch.long).unsqueeze(0).expand(batch_size, -1)
    max_pos = cos.shape[0]
    # FIX #5: Explicit tensor-to-scalar conversion
    # if int(position_ids.max().item()) >= int(max_pos):
    # if (position_ids >= max_pos).any():
    #     max_pid = position_ids.max().item()
    #     raise ValueError(f"position_ids contains values ({int(position_ids.max().item())}) >= max_position_embeddings ({int(max_pos)})")

    if (position_ids >= max_pos).any():
        max_pid = position_ids.max().item()
        raise ValueError(
            f"position_ids contains values ({max_pid}) >= max_position_embeddings ({max_pos})"
        )


    cos_pos = cos[position_ids].unsqueeze(1)
    sin_pos = sin[position_ids].unsqueeze(1)
    q_embed = (q * cos_pos) + (rotate_half(q) * sin_pos)
    k_embed = (k * cos_pos) + (rotate_half(k) * sin_pos)
    return q_embed, k_embed

def rotate_half(x):
    x1 = x[..., :x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2:]
    return torch.cat((-x2, x1), dim=-1)

"""## GQAAttention: Grouped-Query Attention Module

This module implements the **core attention mechanism** of the H64LM model ‚Äî a *Grouped-Query / Grouped-Key-Value Attention (GQA)* layer that combines **multi-head attention**, **rotary embeddings (RoPE)**, **ALiBi bias**, **sliding-window masking**, and **optional FlashAttention** acceleration.

---

### Key Components
- **Query, Key, Value projections:**  
  Four linear layers project hidden states into multi-head representations (`q_proj`, `k_proj`, `v_proj`, `o_proj`).
- **Rotary Position Embeddings (RoPE):**  
  Injects positional information into Q/K using continuous rotations.
- **ALiBi bias (optional):**  
  Adds a distance-based bias that enables extrapolation to longer sequences without position embeddings.
- **Sliding Window Attention:**  
  Restricts attention span for efficiency and stability ‚Äî useful in long-sequence modeling.
- **Attention Sinks:**  
  Keeps a few prefix tokens (‚Äúsink positions‚Äù) always visible to stabilize early-token dependencies.
- **FlashAttention (optional):**  
  Uses GPU-optimized kernels for efficient softmax attention when available.
- **Dropout:**  
  Applied to both attention weights and output projections for regularization.

---

### Parameters
| Parameter | Description |
|------------|-------------|
| `config` | Instance of `H64LMConfig` containing model hyperparameters. |
| `hidden_states` | Input tensor of shape `(batch, seq_len, hidden_dim)`. |
| `attention_mask` | Optional mask for padding or causal masking. |
| `position_ids` | Absolute token positions for rotary embedding alignment. |
| `past_key_value` | Cached K/V states for fast autoregressive decoding. |
| `use_cache` | If `True`, returns current key/value tensors for reuse. |

---

### Key Implementation Details
- **Depth-aware initialization:**  
  Uses `initialize_weights()` for variance-scaled initialization of all projections.
- **Grouped-Key-Value Attention (GQA):**  
  Reduces memory footprint by sharing keys and values across multiple attention heads (`num_kv_heads < num_heads`).
- **ALiBi Bias Creation:**  
  Computed once at initialization with slopes defined as `m**i`, ensuring decreasing bias per head.
- **Causal + Sliding Mask:**  
  Prevents attending to future tokens and optionally limits backward attention range.

---

### Forward Pass Summary
1. Project inputs into `Q`, `K`, `V`.  
2. Apply RoPE positional encoding to `Q` and `K`.  
3. Append cached `K/V` if decoding.  
4. Compute attention scores with optional ALiBi bias.  
5. Apply causal/sliding masks.  
6. Compute weighted sum over `V` using softmax-normalized attention weights.  
7. Output combined context vector with dropout and linear projection.

---

### Features Summary
| Feature | Description |
|----------|-------------|
| **Rotary Embeddings (RoPE)** | Continuous positional encoding for extrapolation. |
| **ALiBi Attention** | Bias term enabling linear extrapolation. |
| **Sliding Window Masking** | Efficient long-sequence support. |
| **FlashAttention Integration** | Fast GPU kernel acceleration. |
| **Caching Support** | Enables autoregressive decoding. |

---

### Intuition
This implementation blends **modern attention innovations**‚ÄîRoPE, ALiBi, GQA, and FlashAttention‚Äîto achieve:
- Lower memory use  
- Better long-context efficiency  
- Stable FP16 training  
- Compatibility with large-scale distributed setups  

Together, these make `GQAAttention` a highly optimized and modular attention block suitable for modern small-to-mid-size LLMs.

"""

class GQAAttention(nn.Module):
    def __init__(self, config: H64LMConfig):
        super().__init__()
        self.config = config
        self.head_dim = config.head_dim
        self.num_heads = config.num_attention_heads
        self.num_kv_heads = config.num_kv_heads
        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(config.hidden_size, self.num_kv_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(config.hidden_size, self.num_kv_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, config.hidden_size, bias=False)
        self.rotary_emb = RotaryEmbedding(dim=self.head_dim, max_position_embeddings=config.max_position_embeddings, base=config.rope_theta)
        self.dropout = nn.Dropout(config.dropout)
        self.sliding_window_size = config.sliding_window_size

        # FIX #5: Create ALiBi slopes on CPU, let model.to(device) move it
        if config.attention_type == "alibi":
            # Canonical ALiBi slopes use a base of 2**(-8/H)
            m = 2**(-8 / self.num_heads)
            # Slopes are m**i for i in [1, 2, ..., num_heads]
            slopes = torch.tensor([m**i for i in range(1, self.num_heads + 1)], dtype=torch.float32)

            self.register_buffer("alibi_slopes", slopes, persistent=True)

        initialize_weights(self.q_proj, config)
        initialize_weights(self.k_proj, config)
        initialize_weights(self.v_proj, config)
        initialize_weights(self.o_proj, config)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        use_cache: bool = False
                        ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        batch_size, seq_len, _ = hidden_states.shape
        device = hidden_states.device
        # project
        query_states = self.q_proj(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = self.k_proj(hidden_states).view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)
        # expand kv heads if needed
        if self.num_kv_heads < self.num_heads:
            repeat = self.num_heads // self.num_kv_heads
            key_states = key_states.repeat_interleave(repeat, dim=1)
            value_states = value_states.repeat_interleave(repeat, dim=1)
        # compute past length (0 if none)
        past_len = 0
        if past_key_value is not None and past_key_value[0] is not None:
            # past_key_value expected shape: (batch, heads, past_len, head_dim)
            past_k, past_v = past_key_value
            past_len = past_k.size(2)
        # build absolute position ids if not provided (account for past length)
        if position_ids is None:
            # shape (batch, seq_len) with absolute positions [past_len .. past_len + seq_len - 1]
            position_ids = torch.arange(past_len, past_len + seq_len, device=device, dtype=torch.long).unsqueeze(0).expand(batch_size, -1)
        # rotary: compute cos/sin for the full range (past + current)
        total_len = past_len + seq_len
        cos, sin = self.rotary_emb(device, total_len)
        # apply rotary: position_ids are absolute positions (0..total_len-1)
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
        # concat past K/V if present (and ensure dtype match)
        if past_key_value is not None and past_key_value[0] is not None:
            past_k, past_v = past_key_value
            if past_k.dtype != key_states.dtype:
                past_k = past_k.to(key_states.dtype)
                past_v = past_v.to(value_states.dtype)
            key_states = torch.cat([past_k, key_states], dim=2)
            value_states = torch.cat([past_v, value_states], dim=2)
        # try flash attention if configured and safe
        attn_output = None
        if self.config.use_flash_attention and flash_attn_func is not None and attention_mask is None:
            window_size = (self.sliding_window_size if seq_len > self.sliding_window_size else -1, -1)
            try:
                attn_output = flash_attn_func(
                    query_states, key_states, value_states,
                    softmax_scale=1.0 / math.sqrt(self.head_dim),
                    causal=True,
                    dropout_p=self.config.dropout if self.training else 0.0,
                    window_size=window_size
                )
            except Exception as e:
                logger.warning(f"flash_attn_func failed: {e}. Falling back to standard attention.")
                attn_output = None
        if attn_output is None:
            # compute raw scores (batch, heads, q_len, k_len)
            scores = torch.matmul(query_states, key_states.transpose(-1, -2)) / math.sqrt(self.head_dim)
            # ALiBi: add distance-based bias per head
            if self.config.attention_type == "alibi":
                # Build relative distances: shape (q_len, k_len)
                # q_idx = torch.arange(0, seq_len, device=device).view(seq_len, 1)
                q_idx = torch.arange(past_len, past_len + seq_len, device=device).view(seq_len, 1)
                k_idx = torch.arange(0, key_states.size(2), device=device).view(1, -1)
                # absolute distance from query position to each key position, considering past offset:
                # actual key absolute positions = k_idx + (past_len) for the keys that came from current chunk we used relative indexing,
                # but since we've concatenated past_k (length past_len) and current keys, k_idx already indexes [0 .. past_len+seq_len-1]
                rel = (k_idx - q_idx).unsqueeze(0).unsqueeze(0)  # shape (1,1,q_len,k_len)
                slopes = self.alibi_slopes.view(1, -1, 1, 1).to(scores.dtype)  # shape (1, heads, 1, 1)
                # negative slope times distance (farther keys get larger negative bias)
                alibi_bias = -slopes * rel
                scores = scores + alibi_bias
            # add attention_mask (assumed to be broadcastable to scores shape) - convert dtype
            if attention_mask is not None:
                scores = scores + attention_mask.to(scores.dtype)
            # mask_value = torch.finfo(scores.dtype).min
            mask_value = torch.tensor(-1e9, dtype=scores.dtype, device=scores.device)


            # Sliding window mask (causal + windowed): mask keys that are in the future (k_idx > q_idx)
            # or that are farther left than sliding_window_size (q_idx - k_idx > sliding_window_size)
            if seq_len + past_len > self.sliding_window_size:
                # create q and k absolute position indices across the concatenated key length
                q_idx = torch.arange(past_len, past_len + seq_len, device=device).view(seq_len, 1)
                k_idx = torch.arange(0, key_states.size(2), device=device).view(1, -1)
                # mask where key is in future relative to query OR distance > window_size
                # future_mask: k_abs > q_abs  -> should be masked for causal
                future_mask = (k_idx > q_idx)
                # distance mask: q_abs - k_abs > window_size  -> mask keys too far in the past
                distance_mask = (q_idx - k_idx) > self.sliding_window_size
                window_mask = (future_mask | distance_mask).unsqueeze(0).unsqueeze(0)  # (1,1,q_len,k_len)
                window_mask = window_mask.expand(batch_size, self.num_heads, -1, -1)
                scores = scores.masked_fill(window_mask, mask_value)
            # attention sinks: if enabled, allow attention to a small prefix even if window would mask them
            # We implement sinks by unmasking the first `sink_len` keys if configured.
            if self.config.use_attention_sinks:
                sink_len = 4  # keep previous behavior: first 4 positions are sinks
                if key_states.size(2) > sink_len:
                    # unmask first sink_len positions by setting scores[..., :sink_len] unchanged (no-op)
                    # (We ensure earlier masks didn't permanently set them to mask_value by reassigning only where mask applies)
                    # For simplicity we do nothing here because we already built a mask that only masks based on future/distance.
                    pass
            # Softmax and dropout
            # avoid numerical issues when all masked: replace -inf rows with zeros after softmax
            attn_weights = F.softmax(scores, dim=-1)
            # if a row was fully masked (all -inf) softmax gives NaNs; clamp those to zeros
            attn_weights = torch.where(torch.isfinite(attn_weights), attn_weights, torch.zeros_like(attn_weights))
            attn_weights = self.dropout(attn_weights) if self.training else attn_weights
            attn_output = torch.matmul(attn_weights, value_states)
        # reshape back and output projection
        attn_output = attn_output.transpose(1, 2).contiguous().reshape(batch_size, seq_len, self.num_heads * self.head_dim)
        attn_output = self.o_proj(attn_output)
        attn_output = self.dropout(attn_output) if self.training else attn_output
        # prepare present key/values for caching
        present_key_value = None
        if use_cache:
            present_dtype = torch.float16 if self.config.kv_cache_fp16 else key_states.dtype
            present_key_value = (key_states.to(present_dtype), value_states.to(present_dtype))
        return attn_output, present_key_value

"""SwiGLU Feed-Forward Network
Overview
This implements the SwiGLU (Swish-Gated Linear Unit) activation function, a gated feed-forward network architecture that has shown improved performance in transformer models.
Architecture
The SwiGLU consists of three linear projections:

Gate Projection (gate_proj): Projects input from hidden_size ‚Üí expert_hidden_size
Up Projection (up_proj): Projects input from hidden_size ‚Üí expert_hidden_size
Down Projection (down_proj): Projects back from expert_hidden_size ‚Üí hidden_size

Forward Pass

"""

class SwiGLU(nn.Module):
    def __init__(self, config: H64LMConfig):
        super().__init__()
        self.gate_proj = nn.Linear(config.hidden_size, config.expert_hidden_size, bias=False)
        self.up_proj = nn.Linear(config.hidden_size, config.expert_hidden_size, bias=False)
        self.down_proj = nn.Linear(config.expert_hidden_size, config.hidden_size, bias=False)
        self.dropout = nn.Dropout(config.expert_dropout)
        initialize_weights(self.gate_proj, config)
        initialize_weights(self.up_proj, config)
        initialize_weights(self.down_proj, config)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        gate = self.gate_proj(x)
        up = self.up_proj(x)
        act = F.silu(gate) * up
        return self.dropout(self.down_proj(act)) if self.training else self.down_proj(act)

"""## Custom MoE Layer

This layer implements a sparse **Mixture-of-Experts** module with optional FasterMoE acceleration.  
Each token is routed to a small subset of experts, allowing the model to scale capacity without increasing compute for all tokens.

### Key Features
- **FasterMoE Path:** Uses optimized kernels + built-in load-balancing loss when available.
- **Fallback Path:** Manual top-k routing, expert grouping, and scatter-based reconstruction.
- **Experts:** Each expert is a SwiGLU feedforward block.
- **Routing:** Softmax gating with temperature scaling; selects `top_k` experts per token.
- **Aux Losses:** Returns `(balance_loss, diversity_loss, z_loss)` to encourage stable expert usage.

### Behavior
1. Compute routing scores ‚Üí select top-k experts per token  
2. Dispatch tokens to their selected experts  
3. Run each expert once on its token batch  
4. Weight and scatter outputs back to the original sequence order  
5. Apply dropout (training only)

Used in Transformer-MoE architectures to increase parameter count efficiently while keeping per-token compute low.

"""

class CustomMoELayer(nn.Module):
    def __init__(self, config: H64LMConfig):
        super().__init__()
        self.config = config
        if FASTERMOE_AVAILABLE:
            self.moe_layer = MoELayer(
                dim_model=config.hidden_size,
                num_experts=config.num_experts,
                top_k=config.num_experts_per_token,
                experts=[SwiGLU(config) for _ in range(config.num_experts)],
                balance_loss_weight=config.load_balance_loss_coeff
            )
            try:
                self.moe_layer = torch.compile(self.moe_layer)
                logger.info("Successfully compiled MoELayer with torch.compile")
            except Exception as e:
                logger.warning(f"torch.compile failed for MoELayer: {e}. Using uncompiled implementation.")
            self.using_fastermoe = True
        else:
            self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)
            self.experts = nn.ModuleList([SwiGLU(config) for _ in range(config.num_experts)])
            initialize_weights(self.gate, config)
            self.using_fastermoe = False
        self.dropout = nn.Dropout(config.dropout)
        self.temperature = config.moe_temperature

    def forward(self, hidden_states: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        MoE forward with robust sparse dispatcher.
        FIX #13: Efficient gather-expert-scatter pattern for fallback.
        """
        if self.using_fastermoe:
            output, aux_loss = self.moe_layer(hidden_states)
            if isinstance(aux_loss, torch.Tensor):
                aux_triplet = (aux_loss, torch.tensor(0.0, device=aux_loss.device), torch.tensor(0.0, device=aux_loss.device))
            elif isinstance(aux_loss, (tuple, list)) and len(aux_loss) >= 3:
                aux_triplet = tuple(torch.as_tensor(x, device=output.device) for x in aux_loss[:3])
            else:
                aux_triplet = (torch.tensor(0.0, device=output.device), torch.tensor(0.0, device=output.device), torch.tensor(0.0, device=output.device))
            return self.dropout(output) if self.training else output, aux_triplet
        else:
                batch_size, seq_len, hidden_size = hidden_states.shape
                device = hidden_states.device

                # --- 1. Routing ---
                logits = self.gate(hidden_states) / self.temperature
                probs = F.softmax(logits, dim=-1)
                topk_weights, topk_indices = torch.topk(probs, k=self.config.num_experts_per_token, dim=-1)

                # Reshape inputs: (B*S, H)
                flat_hidden = hidden_states.reshape(-1, hidden_size)
                # Reshape routing info: (B*S*K,) where K=num_experts_per_token
                expanded_expert_idx = topk_indices.flatten()
                expanded_weights = topk_weights.flatten()

                # Repeat each token's hidden state K times for gathering
                expanded_hidden = flat_hidden.repeat_interleave(self.config.num_experts_per_token, dim=0)

                # --- 2. Dispatch (Batching via Sort) ---
                # Sort by expert index to group all tokens for the same expert together
                sorted_expert_idx, sort_order = torch.sort(expanded_expert_idx)

                # Apply sort order to hidden states and weights
                sorted_hidden = expanded_hidden[sort_order]
                sorted_weights = expanded_weights[sort_order]

                # Find expert segment boundaries to know which contiguous chunk belongs to which expert
                # Use torch.unique_consecutive to handle potential duplicates in unique_experts
                unique_experts, counts = torch.unique_consecutive(sorted_expert_idx, return_counts=True)
                expert_slices = torch.split(sorted_hidden, counts.tolist())
                weight_slices = torch.split(sorted_weights, counts.tolist())

                # --- 3. Expert Execution and Scatter ---
                flat_output = torch.zeros_like(flat_hidden)
                offset = 0

                # Loop only over *active* experts (unique_experts list)
                for expert_idx, expert_input, w in zip(unique_experts.tolist(), expert_slices, weight_slices):
                    # 3a. Expert computation (single large kernel launch per active expert)
                    expert_out = self.experts[expert_idx](expert_input)

                    # 3b. Apply weight
                    weighted_out = expert_out * w.unsqueeze(-1)

                    # 3c. Scatter back using the original token order (sort_order indices)
                    # The index for flat_output is the sort_order slice corresponding to the current expert segment
                    # flat_output.index_add_(0, sort_order[offset:offset+len(w)], weighted_out)
                    expanded_idx_slice = sort_order[offset:offset+len(w)]

                    # Correct Fix: Map the expanded index back to the original token index (B*S)
                    # This uses integer division by K (num_experts_per_token)
                    orig_idx_slice = torch.div(expanded_idx_slice, self.config.num_experts_per_token, rounding_mode="floor").long().to(device)


                    flat_output.index_add_(0, orig_idx_slice, weighted_out)

                    offset += len(w)

                output = flat_output.view(batch_size, seq_len, hidden_size)

                # --- 4. Auxiliary Losses (Same as before) ---
                flattened_probs = probs.reshape(-1, self.config.num_experts)
                mean_usage = flattened_probs.mean(dim=0)
                # Load Balance Loss (L_aux)
                # load_balance_loss = torch.var(mean_usage)
                load_balance_loss = torch.var(mean_usage, unbiased=False)

                # Diversity Loss (L_div)
                marginal = mean_usage / (mean_usage.sum() + 1e-9)
                diversity_loss = -(marginal * torch.log(marginal + 1e-9)).sum()
                # Z-loss (L_z)
                # z_loss = (logits ** 2).mean()
                # z_loss = (logits ** 2).mean(dim=-1).mean()
                z_loss = (logits.float().to(output.device) ** 2).mean(dim=-1).mean()
                aux_loss = (load_balance_loss, diversity_loss, z_loss)

                output = self.dropout(output) if self.training else output
                return output, aux_loss

"""## Gradient Checkpoint Wrapper

Utility to safely wrap a module with PyTorch gradient checkpointing.  
This reduces memory usage by re-computing activations during backward, while avoiding common failure cases with keyword arguments.

### What It Does
- Wraps a module so its forward pass is executed through `torch.utils.checkpoint`.
- Supports non-tensor `kwargs` by capturing them inside the checkpointed closure.
- Rejects tensor kwargs (e.g., `attention_mask`), which PyTorch checkpointing cannot handle.

### Notes
- Uses `use_reentrant=False` when available for safer checkpointing.
- Falls back to legacy reentrant mode if needed.
- Returns the original module unchanged when checkpointing is disabled.

### Why This Exists
Standard checkpointing only accepts positional tensor arguments.  
This wrapper allows modules to still use keyword arguments internally without breaking backward passes.

"""

def checkpoint_forward(module: Callable, use_checkpoint: bool) -> Callable:
    """
    FIX #12/#30: Safer wrapper for gradient checkpointing using a nested function.

    This pattern ensures that keyword arguments (kwargs) are correctly captured
    and passed to the module *inside* the checkpointed function call, allowing
    torch.utils.checkpoint to manage the positional tensor arguments (*args).
    It still rejects tensor kwargs, which is a requirement for standard checkpointing.
    """
    if not use_checkpoint:
        return module

    # The outer wrapped function that handles the arguments and initiates checkpointing
    def wrapped(*args: Tuple[torch.Tensor, ...], **kwargs: Any) -> Any:

        # Guard against tensor keyword arguments, which typically break checkpointing
        if any(isinstance(v, torch.Tensor) for v in kwargs.values()):
            raise ValueError(
                "checkpoint_forward does not support tensor kwargs (e.g., 'attention_mask', 'position_ids') "
                "when wrapping modules; please ensure all tensors required for backward pass are passed as positional arguments."
            )

        # The inner function required by torch.utils.checkpoint.checkpoint.
        # It takes the positional tensor arguments (*inner_args) and forwards
        # the captured, non-tensor kwargs to the original module.
        def inner(*inner_args: Tuple[torch.Tensor, ...]) -> Any:
            return module(*inner_args, **kwargs)

        # Use the newer, safer use_reentrant=False if supported, otherwise fall back.
        try:
            return torch.utils.checkpoint.checkpoint(inner, *args, use_reentrant=False)
        except TypeError:
            return torch.utils.checkpoint.checkpoint(inner, *args)

    return wrapped

"""## H64LMLayer ‚Äî Transformer Block (Pre-Norm)

A single Transformer layer combining attention + feedforward, with optional MoE experts.  
Implements a *pre-norm* architecture and supports selective gradient checkpointing.

### Components
- **Self-Attention:** `GQAAttention` module  
- **Feedforward:** `CustomMoELayer` (sparse) or `SwiGLU` (dense fallback)  
- **Norms:** RMSNorm before attention and before MLP  
- **Skip Connections:** Residuals around both sublayers

### Checkpointing Logic
- Enabled only during training  
- Disabled when the layer uses MoE (saves dispatch correctness)  
- Never applied to attention when `use_cache=True`  
- Uses `checkpoint_forward` to safely wrap submodules

### Forward Flow
1. Pre-attention norm ‚Üí run attention (+ cache)  
2. Add residual  
3. Pre-MLP norm ‚Üí run MLP (MoE or SwiGLU)  
4. Add residual  
5. Return `(hidden_states, aux_loss, present_key_value)`

### Purpose
Defines one scalable Transformer block with MoE-aware routing, stable normalization, and memory-efficient training.

"""

class H64LMLayer(nn.Module):
    def __init__(self, config: H64LMConfig, is_dense_layer: bool = False, layer_idx: int = 0):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.is_dense_layer = is_dense_layer
        self.self_attn = GQAAttention(config)
        self.mlp = CustomMoELayer(config) if not is_dense_layer else SwiGLU(config)
        # FIX #13: Clearer pre-norm architecture
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.layer_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        use_cache: bool = False
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)

        # use_ckpt = self.config.gradient_checkpointing and self.training and not self.is_dense_layer
        # in H64LMLayer.forward, replace use_ckpt assignment with:
        # use_ckpt = self.config.gradient_checkpointing and self.training and not self.is_dense_layer and not isinstance(self.mlp, CustomMoELayer)

        use_ckpt = (
            self.config.gradient_checkpointing
            and self.training
            and not self.is_dense_layer
            and not isinstance(self.mlp, CustomMoELayer)
       )

        attn_ckpt = use_ckpt and (not use_cache)  # Do NOT checkpoint attention if caching is enabled
        attn_callable = checkpoint_forward(self.self_attn, attn_ckpt)


        attn_output, present_key_value = attn_callable(hidden_states, attention_mask, position_ids, past_key_value, use_cache)
        hidden_states = residual + attn_output

        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        mlp_callable = checkpoint_forward(self.mlp, use_ckpt)
        result = mlp_callable(hidden_states)

        if isinstance(result, tuple):
            mlp_output, aux_loss = result
        else:
            mlp_output = result
            aux_loss = (torch.tensor(0.0, device=mlp_output.device), torch.tensor(0.0, device=mlp_output.device), torch.tensor(0.0, device=mlp_output.device))

        hidden_states = residual + mlp_output
        return (hidden_states, aux_loss, present_key_value if use_cache else None)

"""## H64LMModel ‚Äî Core Transformer Backbone

**H64LMModel** defines the main transformer body for H64LM, stacking alternating **dense** and **sparse/MoE** layers with shared embedding and RMS normalization.  
It produces final hidden states used by downstream heads (e.g., language modeling).

---

### Key Components
- **Embedding:** Token embedding layer (`embed_tokens`) projecting input IDs to `hidden_size`.  
- **Alternating Layers:** `H64LMLayer` blocks alternate between dense and sparse modes (`is_dense_layer = i % 2 == 0`).  
- **Normalization:** Final `RMSNorm` applied to output activations.  
- **Initialization:** Depth-aware `initialize_weights()` ensures stable variance scaling.  
- **Checkpointing:** Optional gradient checkpointing for VRAM-efficient training.  
- **Attention Masking:** Mixed-precision‚Äìsafe combination of causal and padding masks (`-1e9` sentinel).

---

### Forward Summary
1. Embed `input_ids` ‚Üí hidden states.  
2. Prepare causal + padding attention mask.  
3. Iterate through transformer layers, caching K/V if `use_cache=True`.  
4. Collect optional MoE load-balance losses.  
5. Apply final RMSNorm and return hidden states, losses, and cache.

"""

class H64LMModel(nn.Module):
    def __init__(self, config: H64LMConfig):
        super().__init__()
        self.config = config
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.layers = nn.ModuleList([
            H64LMLayer(config, is_dense_layer=(i % 2 == 0), layer_idx=i) for i in range(config.num_layers)
        ])
        self.norm = RMSNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.gradient_checkpointing = True
        initialize_weights(self.embed_tokens, config)

        for i, layer in enumerate(self.layers):
            for module in layer.modules():
                if isinstance(module, (nn.Linear, nn.Embedding, RMSNorm)):
                    initialize_weights(module, config, depth=i + 1)

    def enable_gradient_checkpointing(self):
        self.gradient_checkpointing = True
        for layer in self.layers:
            layer.config.gradient_checkpointing = True

    def _prepare_attention_mask(self, attention_mask: Optional[torch.Tensor], batch_size: int, query_len: int, device: torch.device, dtype: torch.dtype, key_len: Optional[int] = None) -> torch.Tensor:
        """
        Build the combined causal + padding mask for attention.

        The key improvement is accepting 'dtype' to ensure compatibility with
        mixed-precision training (e.g., bfloat16) and using a finite sentinel value.
        """
        if key_len is None:
            key_len = query_len

        # PATCH FIX: Use the consistent and numerically safe finite sentinel value.
        # This aligns with the value used in GQAAttention.forward for sliding window masking.
        # We must cast the value to the correct dtype before multiplication.
        mask_value = torch.tensor(-1e9, dtype=dtype, device=device)

        # 1. Create the Causal Mask: (1, 1, QL, KL)
        # Mask values in the upper-right triangle (future positions).
        causal_mask = torch.triu(
            torch.full((query_len, key_len), mask_value, device=device, dtype=dtype),
            diagonal=1
        )
        # Unsqueeze for broadcasting over Batch and Heads
        final_mask = causal_mask.unsqueeze(0).unsqueeze(0)

        # 2. Incorporate the Padding Mask
        if attention_mask is not None:
            # Convert user-provided attention mask (1=real, 0=pad) to a bias mask (0=real, mask_value=pad)
            # Reshape to (B, 1, 1, KL) to broadcast correctly over QL
            padding_mask = (1.0 - attention_mask.to(dtype).view(batch_size, 1, 1, key_len)) * mask_value

            # Combine causal and padding masks using broadcasting
            final_mask = final_mask + padding_mask

        return final_mask

    def forward(
        self,
        input_ids: torch.LongTensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
        use_cache: Optional[bool] = None
    ) -> Tuple[torch.Tensor, List[torch.Tensor], Optional[List[Tuple[torch.Tensor, torch.Tensor]]]]:
        # FIX #14/#15: Removed internal autocast, fixed past_len check
        if use_cache is None:
            use_cache = self.config.use_cache

        batch_size, seq_len = input_ids.shape
        device = input_ids.device

        if position_ids is None:
            position_ids = torch.arange(seq_len, dtype=torch.long, device=device).unsqueeze(0).expand(batch_size, -1)

        # TODO change this back to oregnal
        # hidden_states = self.embed_tokens(input_ids)
        hidden_states = self.embed_tokens(input_ids.long())

        # FIX #15: Safe past_len extraction
        past_len = 0
        if past_key_values is not None and past_key_values[0] is not None:
            past_len = past_key_values[0][0].size(2)

        key_len = past_len + seq_len
        if attention_mask is not None:
            attention_mask = self._prepare_attention_mask(attention_mask, batch_size, seq_len, device,  dtype=hidden_states.dtype, key_len=key_len)

        if past_key_values is None and use_cache:
            past_key_values = [None] * len(self.layers)

        next_cache = [] if use_cache else None
        load_balance_losses = []

        for i, layer in enumerate(self.layers):
            past_key_value = past_key_values[i] if past_key_values is not None else None
            layer_outputs = layer(
                hidden_states, attention_mask, position_ids, past_key_value, use_cache
            )
            hidden_states = layer_outputs[0]
            load_balance_loss = layer_outputs[1]
            if load_balance_loss is not None:
                load_balance_losses.append(load_balance_loss)
            if use_cache:
                next_cache.append(layer_outputs[2])

        hidden_states = self.norm(hidden_states)
        outputs = (hidden_states, load_balance_losses)
        if use_cache:
            outputs += (next_cache,)
        return outputs

"""## H64LMForCausalLM ‚Äî Causal Language Model Head

**H64LMForCausalLM** is the full language model stack built on top of the `H64LMModel` backbone. It is responsible for the final next-token prediction via the language modeling head and the crucial aggregation of all training losses, including the auxiliary MoE terms.

---

### `forward()` Method: Training & Loss Calculation

The forward pass is optimized for efficient causal language modeling training:

* **Prediction Head:** A non-biased linear layer (`lm_head`) projects the hidden states to the vocabulary size (`vocab_size`).
* **Causal Loss Fix:** Implements the **critical shifting** of logits and labels (`logits[..., :-1, :]` vs. `labels[..., 1:]`) to ensure correct next-token prediction.
* **Loss Aggregation:** Calculates the core **Cross-Entropy Loss (CE)** and, if the model is in training mode (`self.training`), aggregates it with the three **Mixture-of-Experts (MoE) Auxiliary Losses**:
    1.  **Load Balance Loss** (`load_balance_loss_coeff`)
    2.  **Diversity Loss** (`diversity_loss_coeff`)
    3.  **Z-Loss** (`z_loss_coeff`)

---

### `generate()` Method: Efficient Text Generation

This method provides a robust, batched interface for text generation leveraging the attention Key/Value (KV) cache:

* **KV-Cache Decoding:** Iteratively generates tokens, passing `past_key_values` (KV-Cache) to restrict the forward pass to the current token only for massive speed-up.
* **Robust Sampling:** Supports advanced decoding strategies:
    * **Temperature Scaling:** Controls randomness/creativity.
    * **Top-K Filtering:** Restricts sampling to the $k$ most likely tokens.
    * **Top-P (Nucleus) Sampling:** Restricts sampling to the smallest set of tokens whose cumulative probability exceeds $p$.
* **Repetition Penalty:** Includes a basic penalty mechanism (`logits[token_id] /= 1.2`) to discourage immediate token repetition.
* **Early Stopping:** Checks for the End-of-Sequence (`eos_token_id`) to terminate generation.
"""

class H64LMForCausalLM(nn.Module):
    def __init__(self, config: H64LMConfig, tokenizer=None):
        super().__init__()
        self.config = config
        self.model = H64LMModel(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.tokenizer = tokenizer
        self._warned_vllm = False
        initialize_weights(self.lm_head, config)
        if config.tie_word_embeddings:
            self.lm_head.weight = self.model.embed_tokens.weight

    def forward(self, input_ids, attention_mask=None, position_ids=None,
                                past_key_values=None, labels=None, use_cache=None):
        """
        FIXED: Proper loss calculation in H64LMForCausalLM.forward()

        Key changes:
        - Shift logits and labels correctly for next-token prediction
        - Handle variable sequence lengths properly
        - Use ignore_index=-100 for masked tokens
        """
        device = input_ids.device

        outputs = self.model(input_ids, attention_mask, position_ids, past_key_values, use_cache)
        hidden_states = outputs[0]
        load_balance_losses = outputs[1]
        logits = self.lm_head(hidden_states)

        loss = None
        if labels is not None:
            # CRITICAL FIX: Proper shifting for causal LM
            # We predict token i+1 from tokens 0..i
            # So logits[..., :-1, :] predicts labels[..., 1:]

            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()

            # Use ignore_index=-100 to skip masked tokens (padding, etc.)
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')
            ce_loss = loss_fct(
                shift_logits.view(-1, shift_logits.size(-1)),
                shift_labels.view(-1)
            )

            total_loss = ce_loss

            # Only add auxiliary losses during training
            if self.training and load_balance_losses:
                lb_loss = sum([l[0] for l in load_balance_losses]) / len(load_balance_losses)
                div_loss = sum([l[1] for l in load_balance_losses]) / len(load_balance_losses)
                z_loss = sum([l[2] for l in load_balance_losses]) / len(load_balance_losses)

                total_loss += self.config.load_balance_loss_coeff * lb_loss
                total_loss += self.config.diversity_loss_coeff * div_loss
                total_loss += self.config.z_loss_coeff * z_loss

            loss = total_loss

        return {
            'loss': loss,
            'logits': logits,
            'past_key_values': outputs[2] if len(outputs) > 2 else None,
            'load_balance_losses': load_balance_losses,
        }


    def generate(
        self,
        input_ids: torch.LongTensor,
        attention_mask: Optional[torch.Tensor] = None,
        max_new_tokens: int = 50,
        temperature: float = 0.8,
        top_p: float = 0.9,
        top_k: int = 50,
        do_sample: bool = True
    ) -> torch.LongTensor:
        """
        FIXED: Improved text generation with better sampling.

        Args:
            input_ids: Input token IDs (batch_size, seq_len)
            attention_mask: Attention mask (batch_size, seq_len)
            max_new_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            top_p: Nucleus sampling threshold
            top_k: Top-k sampling threshold
            do_sample: Whether to use sampling (vs greedy)

        Returns:
            Generated token IDs (batch_size, seq_len + max_new_tokens)
        """
        self.eval()
        device = input_ids.device
        batch_size, seq_len = input_ids.shape

        if attention_mask is None:
            attention_mask = torch.ones((batch_size, seq_len), device=device)

        generated = input_ids
        past_key_values = None
        generated_tokens = set()

        with torch.no_grad():
            for step in range(max_new_tokens):
                # Forward pass
                outputs = self(
                    input_ids=input_ids if past_key_values is None else input_ids[:, -1:],
                    attention_mask=attention_mask,
                    past_key_values=past_key_values,
                    use_cache=True
                )

                logits = outputs['logits'][:, -1, :]
                past_key_values = outputs['past_key_values']

                if do_sample:
                    # Apply repetition penalty
                    for token_id in generated_tokens:
                        logits[0, token_id] /= 1.2

                    # Temperature scaling
                    logits = logits / temperature

                    # Top-k filtering
                    if top_k > 0:
                        indices_to_remove = logits < torch.topk(logits, min(top_k, logits.size(-1)))[0][..., -1, None]
                        logits[indices_to_remove] = float('-inf')

                    # Top-p (nucleus) filtering
                    if top_p < 1.0:
                        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
                        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

                        sorted_indices_to_remove = cumulative_probs > top_p
                        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                        sorted_indices_to_remove[..., 0] = 0

                        indices_to_remove = sorted_indices_to_remove.scatter(
                            1, sorted_indices, sorted_indices_to_remove
                        )
                        logits[indices_to_remove] = float('-inf')

                    # Sample
                    probs = F.softmax(logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    # Greedy
                    next_token = torch.argmax(logits, dim=-1, keepdim=True)

                # Check for EOS
                if next_token.item() == self.config.eos_token_id:
                    break

                generated_tokens.add(next_token.item())
                generated = torch.cat([generated, next_token], dim=1)
                input_ids = next_token
                attention_mask = torch.cat([attention_mask, torch.ones(batch_size, 1, device=device)], dim=1)

        return generated

"""## DedupPipeline ‚Äî Dataset Curation and Deduplication Utility

The **DedupPipeline** is a robust preprocessing tool for generating clean, unique, and linguistically diverse text corpora for Large Language Model (LLM) pretraining. It combines streaming ingestion with strict quality filters to maximize data efficiency and model generalization.

---

### Core Workflow

The pipeline executes a memory-efficient, multi-step filtration process:

1.  **Streaming Ingestion:** Loads datasets using `streaming=True`, handling vast data volumes with minimal memory overhead, limited by `max_examples`.
2.  **Document Deduplication:** Enforces **global uniqueness** by tracking document hashes (SHA-256) in `self.seen_hashes`.
3.  **Text Quality Filters:**
    * **Length Check:** Discards texts shorter than 50 characters.
    * **Word Diversity:** Filters out boilerplate or repetitive content where **(unique words / total words) < 0.3**.

---

### Final Metrics and Output

* **Tokenization:** Safely tokenizes accepted texts up to `max_length` (e.g., 2048), handling various tensor/list outputs.
* **Token Diversity Metric:** Calculates the ratio of unique token IDs to total tokens.
* **Final Acceptance:** Only samples meeting the final quality threshold (**diversity score $\geq 0.15$**) are included.

**Output:** Returns a list of dictionaries containing the high-quality, pre-tokenized `input\_ids` ready for training.
"""

class DedupPipeline:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self.seen_hashes = set()

    def process(self, sources: List[str] = ['wikitext'], max_length: int = 2048, max_examples: int = 10000) -> List[Dict]:
        # FIX #18: Safer tokenization
        filtered = []
        count = 0
        for source in sources:
            try:
                dataset = load_dataset(source, split='train', streaming=True)
            except Exception as e:
                logger.warning(f"Failed to load {source}: {e}")
                continue
            for example in dataset:
                if count >= max_examples:
                    break
                text = example.get('text', '')
                if not isinstance(text, str) or len(text) < 50:
                    continue
                if len(set(text.lower().split())) / len(text.split()) < 0.3:
                    continue
                h = hashlib.sha256(text.encode('utf-8')).hexdigest()
                if h not in self.seen_hashes:
                    enc = self.tokenizer(text, truncation=True, max_length=max_length)
                    tokens = enc["input_ids"]
                    if isinstance(tokens, list):
                        pass
                    elif isinstance(tokens, torch.Tensor):
                        tokens = tokens[0].tolist() if tokens.dim() > 1 else tokens.tolist()
                    else:
                        continue
                    if len(tokens) < 10:
                        continue
                    unique_tokens = len(set(tokens))
                    # diversity_score = len(tokens) / (unique_tokens + 1)
                    diversity_score = unique_tokens / len(tokens)
                    if diversity_score >= 0.15:
                        filtered.append({'text': text, 'input_ids': tokens})
                        self.seen_hashes.add(h)
                    count += 1
        return filtered

"""## train_tokenizer ‚Äî Byte-Pair Encoding (BPE) Tokenizer Trainer

The **train_tokenizer** function builds a robust BPE tokenizer directly from a text dataset, supporting both **standard** and **streaming** Hugging Face datasets.  
It extracts a high-quality text sample, trains a subword tokenizer, and exports it for reuse in LLM pretraining workflows.

---

### Core Workflow

1. **Dataset Handling:**  
   - Supports both `Dataset` and `DatasetDict` formats.  
   - Automatically selects the `'train'` split if present.  
   - Handles streaming datasets gracefully when random access (`len()`) is unavailable.

2. **Sampling:**  
   - Collects up to **100,000 text samples** (minimum 20 characters).  
   - Falls back to streaming iteration when slicing isn‚Äôt supported.  
   - Logs a warning if fewer than **1,000 valid samples** are found.

3. **Tokenizer Initialization:**  
   - Uses `tokenizers` library with **Byte-Pair Encoding (BPE)** model.  
   - Applies **ByteLevel pre-tokenization** for consistent whitespace handling.  
   - Defines key special tokens: `[PAD]`, `[BOS]`, `[EOS]`, `[UNK]`.

4. **Training & Saving:**  
   - Trains using `BpeTrainer` with target vocabulary size (`vocab_size`, default 32k).  
   - Saves the final tokenizer JSON to the specified `save_path` (default: `h64lm_tokenizer.json`).

---

### Output
A serialized **BPE tokenizer file** (`.json`) compatible with Hugging Face‚Äôs `PreTrainedTokenizerFast` API, ready for integration into the H64LM pretraining pipeline.

"""

def train_tokenizer(dataset, vocab_size=32000, save_path="h64lm_tokenizer.json"):
    """Train a BPE tokenizer on the dataset (supports both normal and streaming datasets)."""
    logger.info("Training tokenizer...")

    texts = []
    sample_size = 100000
    count = 0

    # Pick a split if dataset is a DatasetDict
    if hasattr(dataset, 'keys'):  # DatasetDict
        dataset_iter = dataset['train']
    else:
        dataset_iter = dataset

    # Handle regular vs streaming datasets
    try:
        iterable = dataset_iter.select(range(min(sample_size, len(dataset_iter))))
    except (TypeError, AttributeError):
        iterable = dataset_iter  # streaming mode fallback

    for ex in iterable:
        if isinstance(ex, dict):
            text = str(ex.get("text", "")).strip()
        else:
            text = str(ex).strip()
        if len(text) > 20:
            texts.append(text)
            count += 1
            if count >= sample_size:
                break

    if len(texts) < 1000:
        logger.warning(f"Only {len(texts)} valid texts found for tokenizer training")

    tokenizer = Tokenizer(models.BPE())
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)
    trainer = trainers.BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=["[PAD]", "[BOS]", "[EOS]", "[UNK]"]
    )

    tokenizer.train_from_iterator(texts, trainer=trainer)
    tokenizer.save(save_path)
    logger.info(f"Tokenizer saved to {save_path}")

"""## diagnose_training_data ‚Äî Tokenizer and Dataset Sanity Checker

The **diagnose_training_data** utility performs a quick interactive diagnostic on the prepared training dataset and tokenizer configuration.  
It helps ensure tokenization, special tokens, and sample integrity are correct **before launching pretraining**.

---

### Core Workflow

1. **Tokenizer Inspection:**  
   - Displays key tokenizer metadata, including:  
     - Vocabulary size (`vocab_size`)  
     - PAD, BOS, and EOS tokens with their corresponding IDs  

2. **Dataset Sampling:**  
   - Selects up to `num_samples` examples from the training dataset (default: **5**).  
   - Truncates each text to 200 characters for readability.  
   - Converts the text to token IDs using the provided tokenizer.

3. **Per-Sample Diagnostics:**  
   For each example:
   - Prints the raw text snippet.  
   - Displays the first 20 token IDs.  
   - Decodes and prints the corresponding tokens.  
   - Checks if special tokens (like PAD) appear in the sequence.  
   - Reports the total token count.

---

### Purpose

- Verify that tokenizer is correctly trained and initialized.  
- Confirm that dataset samples are properly tokenizable.  
- Quickly identify formatting or tokenization errors before training.

---

### Output

Human-readable console output summarizing:
- Tokenizer configuration  
- Example tokenizations  
- Special token presence  
- Sequence lengths

"""

def diagnose_training_data(tokenizer, train_dataset, num_samples=5):
    """
    Diagnostic function to check if training data is properly prepared.
    """
    print("\n=== TRAINING DATA DIAGNOSTICS ===")
    print(f"Tokenizer vocab size: {tokenizer.vocab_size}")
    print(f"PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})")
    print(f"EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})")
    print(f"BOS token: '{tokenizer.bos_token}' (ID: {tokenizer.bos_token_id})")

    print(f"\n=== Sample {num_samples} Training Examples ===")
    for i in range(min(num_samples, len(train_dataset))):
        example = train_dataset[i]
        text = example["text"][:200]  # First 200 chars

        # Tokenize
        tokens = tokenizer(text, return_tensors="pt")
        token_ids = tokens["input_ids"][0].tolist()

        print(f"\nExample {i+1}:")
        print(f"Text: {text}...")
        print(f"Token IDs: {token_ids[:20]}...")
        print(f"Decoded: {tokenizer.decode(token_ids[:20])}...")

        # Check for special tokens
        print(f"Contains PAD: {tokenizer.pad_token_id in token_ids}")
        print(f"Token count: {len(token_ids)}")

"""## create_collate_fn ‚Äî Dynamic Collation Factory for Language Model Training

The **create_collate_fn** function constructs a **closure-based collation utility** that prepares tokenized batches for causal language model (CLM) training.  
It ensures alignment between tokenization, padding, and label masking‚Äîcritical for stable and accurate model optimization.

---

### Core Workflow

1. **Factory Closure:**
   - Captures both `tokenizer` and `config` objects within a closure.
   - Returns a ready-to-use `collate_fn` compatible with PyTorch‚Äôs `DataLoader`.

2. **Batch Tokenization:**
   - Extracts `"text"` fields from the input batch.
   - Tokenizes all texts simultaneously with:
     - Automatic padding (`padding=True`)
     - Length truncation (`truncation=True`)
     - Maximum sequence length defined by `config.max_position_embeddings`

3. **Tensor Preparation:**
   - Produces `input_ids` and `attention_mask` tensors in batch form (`return_tensors="pt"`).

4. **Label Generation and Masking:**
   - Clones `input_ids` to create `labels`.
   - Replaces padding token IDs with **-100**, ensuring they‚Äôre ignored by `CrossEntropyLoss`.
   - This aligns the model‚Äôs loss computation strictly to real text tokens.

---

### Purpose

- Provides a clean, reusable batch preprocessing function for **causal language modeling**.  
- Guarantees consistent token alignment between inputs and labels.  
- Prevents padding tokens from contributing to the loss, improving training stability and convergence.

---

### Output

Returns a dictionary suitable for direct input into a Hugging Face‚Äìstyle model forward pass:

```python
{
    "input_ids": LongTensor,        # Tokenized input sequences
    "attention_mask": LongTensor,   # Binary mask for padded tokens
    "labels": LongTensor            # Input IDs with pads replaced by -100
}

"""

def create_collate_fn(tokenizer, config):
    """
    Factory function to create collate_fn with proper closure.
    This captures tokenizer and config in the closure so DataLoader can call it.
    """
    def collate_fn(batch):
        """
        Collate function with proper label masking for causal language modeling.

        - Labels are input_ids with padding tokens masked as -100
        - Ensures loss is only computed on actual text tokens
        """
        texts = [item["text"] for item in batch]

        # Tokenize with proper padding
        encodings = tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=config.max_position_embeddings
        )

        input_ids = encodings["input_ids"].long()
        attention_mask = encodings["attention_mask"]

        # CRITICAL: Create labels by cloning input_ids
        labels = input_ids.clone()

        # Mask padding tokens with -100 (ignored in CrossEntropyLoss)
        labels[labels == tokenizer.pad_token_id] = -100

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }

    return collate_fn

"""## train_model ‚Äî Full-Featured Training Loop for Transformer-Based LMs

The **train_model** function implements a complete, reproducible, and fault-tolerant training pipeline for transformer-based language models.  
It integrates diagnostic checks, gradient scaling, checkpointing, and evaluation into a unified, research-ready training routine.

---

### Core Workflow

#### 1. **Initialization and Diagnostics**
- Runs `diagnose_training_data()` to verify tokenizer and dataset readiness.
- Creates checkpoint directories and identifies device (`cuda` or `cpu`).
- Normalizes model reference for both wrapped and unwrapped (DDP/DataParallel) cases.

#### 2. **Data Preparation**
- Constructs a **closure-based `collate_fn`** via `create_collate_fn()` to ensure consistent tokenization, padding, and label masking.
- Converts list-based datasets into Hugging Face `Dataset` objects if needed.
- Builds efficient PyTorch **DataLoaders** with pinned memory and configurable workers.

#### 3. **Optimizer and Scheduler Setup**
- Uses `AdamW` with parameter grouping:
  - **Weight Decay:** Applied only to non‚ÄìLayerNorm and non-bias parameters.  
  - **No Decay:** For normalization and bias terms.
- Linear learning-rate schedule with 10% warmup steps.
- Enables **mixed-precision training** via `GradScaler`.

---

### Training Loop

For each epoch:
1. Iterates through batches using **gradient accumulation** (`grad_accum_steps`) to simulate large effective batch sizes.
2. Applies:
   - Mixed-precision autocasting
   - Gradient clipping (`max_norm=1.0`)
   - GradScaler stepping and unscaling
3. Tracks running loss, smoothed loss, and throughput in tokens/sec.
4. Periodically logs training metrics and learning rate updates.

---

### Validation and Checkpointing

- After each epoch:
  - Runs **validation** using `validate_model()` to compute loss and perplexity.
  - Updates `training_history` with epoch metrics.
- Saves:
  - **Best model checkpoint** (based on lowest validation loss)
  - **Per-epoch checkpoint** for full reproducibility
- Exports `training_history.json` with:
  - Train/validation loss  
  - Perplexity  
  - Learning rate trajectory  
  - Tokens-per-second throughput

---

### Logged Metrics

| Metric | Description |
|:--|:--|
| `train_loss` | Mean loss per epoch on training set |
| `val_loss` | Mean validation loss |
| `val_perplexity` | Exponential of validation loss (clamped to prevent overflow) |
| `learning_rates` | Final LR per epoch from scheduler |
| `tokens_per_second` | Data throughput (training speed) |

---

### Key Implementation Features

- **Robust NaN/Inf filtering** ‚Äî skips unstable batches safely.  
- **Deterministic and resumable** ‚Äî checkpoints include optimizer and scheduler state.  
- **Scalable and GPU-efficient** ‚Äî supports mixed precision and gradient accumulation.  
- **Clear console diagnostics** via `tqdm` progress bars and structured `[INFO]` logs.

---

### Usage Example

```python
training_history = train_model(
    model=model,
    config=config,
    training_config=training_config,
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    tokenizer=tokenizer,
    num_epochs=3,
    batch_size=8,
    grad_accum_steps=2,
    checkpoint_dir="checkpoints_h64lm"
)

"""

def train_model(
    model, config, training_config, train_dataset, val_dataset,
    tokenizer, num_epochs=3, batch_size=4, grad_accum_steps=1,
    log_interval=50, save_interval=500, val_interval=None,
    checkpoint_dir="checkpoints_h64lm", use_tensorboard=False, resume_from=None
):
    """Training loop with ALL fixes applied."""


    # Run diagnostics
    diagnose_training_data(tokenizer, train_dataset)

    # Create checkpoint directory
    os.makedirs(checkpoint_dir, exist_ok=True)

    # Setup device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    raw_model = model.module if hasattr(model, "module") else model
    raw_model.train()

    print(f"[INFO] Training on device: {device}")
    print(f"[INFO] Train dataset size: {len(train_dataset)}")
    print(f"[INFO] Val dataset size: {len(val_dataset)}")

    # FIX: Create collate_fn with proper closure
    collate_fn = create_collate_fn(tokenizer, config)

    # Convert to Dataset if needed
    if isinstance(train_dataset, list):
        train_dataset = Dataset.from_list(train_dataset)
    if isinstance(val_dataset, list):
        val_dataset = Dataset.from_list(val_dataset)

    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,
        shuffle=True,
        num_workers=2,
        pin_memory=(device.type == "cuda"),
        drop_last=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,
        shuffle=False,
        num_workers=2,
        pin_memory=(device.type == "cuda"),
        drop_last=False
    )

    # Setup optimizer
    total_steps = (len(train_loader) * num_epochs) // grad_accum_steps
    base_lr = 3e-4
    no_decay = ["bias", "LayerNorm.weight", "layernorm.weight", "norm.weight"]

    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in model.named_parameters()
                      if not any(nd in n for nd in no_decay) and p.requires_grad],
            "weight_decay": 0.01,
        },
        {
            "params": [p for n, p in model.named_parameters()
                      if any(nd in n for nd in no_decay) and p.requires_grad],
            "weight_decay": 0.0,
        },
    ]

    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=base_lr, betas=(0.9, 0.95), eps=1e-8)
    warmup_steps = max(1, total_steps // 10)
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)
    scaler = GradScaler(enabled=training_config.use_fp16)

    # Training loop
    best_val_loss = float('inf')
    training_history = {
        'train_loss': [],
        'val_loss': [],
        'val_perplexity': [],
        'learning_rates': [],
        'tokens_per_second': []
    }

    for epoch in range(num_epochs):
        print(f"[INFO] ==== Starting epoch {epoch+1}/{num_epochs} ====")

        model.train()
        total_loss = 0.0
        num_batches = 0
        num_tokens = 0
        epoch_start_time = time.time()
        optimizer.zero_grad()
        recent_losses = deque(maxlen=20)

        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f"Epoch {epoch+1}")

        for step, batch in pbar:
            batch = {k: v.to(device) for k, v in batch.items()}

            with autocast(enabled=training_config.use_fp16):
                outputs = model(**batch)
                loss = outputs["loss"]

            if loss is None:
                continue
            if loss.dim() > 0:
                loss = loss.mean()
            if torch.isnan(loss) or torch.isinf(loss):
                continue

            report_loss = loss.item()
            total_loss += report_loss
            num_batches += 1
            recent_losses.append(report_loss)
            num_tokens += batch['input_ids'].numel()

            loss = loss / grad_accum_steps
            scaler.scale(loss).backward()

            if (step + 1) % grad_accum_steps == 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
                scheduler.step()
                optimizer.zero_grad()

            avg_loss = total_loss / num_batches
            smoothed_loss = sum(recent_losses) / len(recent_losses)
            pbar.set_postfix({
                'loss': f'{report_loss:.4f}',
                'smooth': f'{smoothed_loss:.4f}',
                'lr': f'{scheduler.get_last_lr()[0]:.6e}'
            })

        pbar.close()

        # Calculate metrics
        epoch_time = time.time() - epoch_start_time
        avg_train_loss = total_loss / max(1, num_batches)
        train_ppl = np.exp(min(avg_train_loss, 20))
        tokens_per_sec = num_tokens / epoch_time

        print(f"[INFO] Train Loss: {avg_train_loss:.4f}, Train PPL: {train_ppl:.2f}")
        print(f"[INFO] Throughput: {tokens_per_sec:.0f} tokens/sec")

        # FIX: Use validate_model_fixed
        print("[INFO] Running validation...")
        val_loss, val_ppl = validate_model(model, val_loader, device, training_config.use_fp16)

        print(f"[INFO] ==== Epoch {epoch+1} Results ====")
        print(f"[INFO] Val Loss: {val_loss:.4f}, Val PPL: {val_ppl:.2f}")

        training_history['train_loss'].append(avg_train_loss)
        training_history['val_loss'].append(val_loss)
        training_history['val_perplexity'].append(val_ppl)
        training_history['learning_rates'].append(scheduler.get_last_lr()[0])
        training_history['tokens_per_second'].append(tokens_per_sec)

        # Save best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_path = os.path.join(checkpoint_dir, "best_model.pt")
            save_checkpoint(raw_model, optimizer, scheduler, epoch, len(train_loader), best_path)
            print(f"[INFO] ‚úì New best model saved with val loss {best_val_loss:.4f}")

        # Save epoch checkpoint
        checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_epoch{epoch}_final.pt")
        save_checkpoint(raw_model, optimizer, scheduler, epoch, len(train_loader), checkpoint_path)

    # Save history
    history_path = os.path.join(checkpoint_dir, "training_history.json")
    with open(history_path, 'w') as f:
        json.dump(training_history, f, indent=2)

    return training_history

"""## validate_model ‚Äî Robust Evaluation for Language Models

The **validate_model** function performs accurate and stable validation for transformer-based language models.  
It computes **average token-weighted loss** and **perplexity**, with safeguards against invalid batches and NaN/Inf propagation.

---

### Core Workflow

#### 1. **Model Preparation**
- Automatically unwraps `model.module` (for DDP or DataParallel setups).  
- Switches the model to **evaluation mode** (`eval()`) to disable dropout and gradient updates.

#### 2. **Evaluation Loop**
- Iterates through the validation DataLoader with `torch.no_grad()` for efficiency.  
- Moves each batch to the active device (`cuda` or `cpu`).  
- Uses **mixed-precision inference** (`torch.cuda.amp.autocast`) for faster validation on GPUs.  
- Computes the loss from model outputs:
  - Handles missing (`None`) losses gracefully.
  - Reduces multi-GPU tensor losses via mean().
  - Skips invalid (NaN or Inf) values to preserve metric integrity.

#### 3. **Token-Weighted Loss Aggregation**
- Counts **valid (non-masked) tokens**:  
  `valid_tokens = (labels != -100).sum().item()`
- Accumulates weighted losses across all valid tokens for robust averaging.  
- Computes the final **average loss per token**.

---

### Output Metrics

| Metric | Description |
|:--|:--|
| `avg_loss` | Mean token-weighted loss across validation samples |
| `perplexity` | Exponential of average loss, clipped to avoid overflow (`exp(min(loss, 20))`) |

> Returns `(avg_loss, perplexity)` as floats.

---

### Key Implementation Features

- **Token-weighted averaging** ensures accurate loss even for variable-length sequences.  
- **NaN/Inf filtering** prevents metric corruption on unstable batches.  
- **DDP-aware evaluation** handles wrapped models (`model.module`).  
- **FP16-safe autocasting** for efficient GPU validation.  
- **Graceful failure handling** ‚Äî returns `(inf, inf)` if no valid tokens are found.

---

### Example Usage

```python
val_loss, val_ppl = validate_model(
    model=model,
    val_loader=val_loader,
    device=torch.device("cuda"),
    use_fp16=True
)
print(f"Validation Loss: {val_loss:.4f}, Perplexity: {val_ppl:.2f}")

"""

def validate_model(model, val_loader, device, use_fp16=True):
    """
    FIXED: Proper validation with loss calculation.

    Key changes:
    - Ensures model is in eval mode
    - Handles None losses gracefully
    - Computes perplexity safely
    - Better error logging
    """

    raw_model = model.module if hasattr(model, "module") else model
    raw_model.eval()

    total_loss = 0.0
    total_tokens = 0
    num_batches = 0

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validation", leave=False):
            batch = {k: v.to(device) for k, v in batch.items()}

            with autocast(enabled=use_fp16):
                outputs = raw_model(**batch)
                loss = outputs["loss"]

            # Handle None loss
            if loss is None:
                continue

            # Handle multi-GPU (DataParallel returns vector)
            if loss.dim() > 0:
                loss = loss.mean()

            # Skip invalid losses
            if torch.isnan(loss) or torch.isinf(loss):
                continue

            # Count actual tokens (not padding)
            labels = batch.get("labels", batch["input_ids"])
            valid_tokens = (labels != -100).sum().item()

            total_loss += loss.item() * valid_tokens
            total_tokens += valid_tokens
            num_batches += 1

    if total_tokens == 0:
        return float('inf'), float('inf')

    # FIXED: Weight loss by number of tokens
    avg_loss = total_loss / total_tokens
    perplexity = np.exp(min(avg_loss, 20))

    return avg_loss, perplexity

"""## save_checkpoint ‚Äî Safe and Atomic Model Checkpointing

The **save_checkpoint** function securely saves model, optimizer, and scheduler states to disk during training.  
It is designed for **multi-GPU environments**, **safe serialization**, and **crash-resilient saving**, ensuring no corrupted checkpoints.

---

### Core Workflow

#### 1. **State Packaging**
- Collects key training states into a dictionary:
  - `epoch` ‚Äî Current epoch number  
  - `step` ‚Äî Training step within the epoch  
  - `model_state_dict` ‚Äî Model parameters, moved to **CPU** and cloned for safe serialization  
  - `optimizer_state_dict` ‚Äî Optimizer state  
  - `scheduler_state_dict` ‚Äî Learning rate scheduler state  

> Moving model weights to CPU ensures GPU memory safety and cross-device checkpoint loading.

---

#### 2. **Atomic Save Operation**
- Writes the checkpoint to a **temporary file** (`.tmp` suffix) using PyTorch‚Äôs new zipfile-based serialization:  
  ```python
  torch.save(checkpoint, temp_path, _use_new_zipfile_serialization=True)

"""

def save_checkpoint(model, optimizer, scheduler, epoch, step, filepath):
    """Save training checkpoint with error handling for multi-GPU."""
    try:
        # FIX: Ensure we're on CPU and use safe pickling
        checkpoint = {
            'epoch': epoch,
            'step': step,
            'model_state_dict': {k: v.cpu().clone() for k, v in model.state_dict().items()},
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
        }

        # FIX: Use atomic write with temp file
        temp_path = filepath + '.tmp'

        # TODO: Uncomment this
        torch.save(checkpoint, temp_path, _use_new_zipfile_serialization=True)

        # Only rename if save succeeded
        if os.path.exists(temp_path):
            # Remove old file if exists
            if os.path.exists(filepath):
                os.remove(filepath)
            os.rename(temp_path, filepath)
            logger.info(f"Saved checkpoint to {filepath}")
    except Exception as e:
        logger.error(f"Failed to save checkpoint: {e}")
        # Clean up temp file if it exists
        if os.path.exists(filepath + '.tmp'):
            try:
                os.remove(filepath + '.tmp')
            except:
                pass

"""## load_checkpoint_for_resume ‚Äî Robust Checkpoint Loader for Training Resumption

The **load_checkpoint_for_resume** function safely restores training state (model, optimizer, and scheduler) from a checkpoint file.  
It is designed to handle **DataParallel prefixes**, **partial state_dict checkpoints**, and **key mismatches** gracefully ‚Äî ensuring training can resume smoothly without corruption or crashes.

---

### Core Workflow

#### 1. **Checkpoint Loading**
- Loads the checkpoint from disk onto CPU (ensuring cross-device compatibility):
  ```python
  checkpoint = torch.load(checkpoint_path, map_location='cpu')

"""

def load_checkpoint_for_resume(model, optimizer, scheduler, checkpoint_path, device):
    """
    Load checkpoint for resuming training with robust module prefix handling.
    Handles both full checkpoints and state_dict-only files.
    Automatically strips 'module.' prefix from DataParallel checkpoints.
    """
    try:
        print(f"[INFO] Loading checkpoint from: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location='cpu')

        # Get raw model (unwrap DataParallel if needed)
        raw_model = model.module if hasattr(model, 'module') else model

        # Determine checkpoint format
        if 'model_state_dict' in checkpoint:
            # Full checkpoint with optimizer/scheduler
            state_dict = checkpoint['model_state_dict']
        else:
            # State dict only (like your best_model_state_dict.pt)
            state_dict = checkpoint

        # ========== ROBUST FIX: Always strip 'module.' prefix ==========
        # raw_model never has 'module.' prefix, so we strip it from checkpoint
        print(f"[INFO] Original checkpoint keys (first 3): {list(state_dict.keys())[:3]}")

        new_state_dict = {}
        for k, v in state_dict.items():
            # Remove 'module.' prefix if present
            if k.startswith('module.'):
                new_key = k[7:]  # Remove 'module.' (7 characters)
            else:
                new_key = k
            new_state_dict[new_key] = v

        print(f"[INFO] Cleaned checkpoint keys (first 3): {list(new_state_dict.keys())[:3]}")

        # Get expected keys from current model
        model_keys = set(raw_model.state_dict().keys())
        checkpoint_keys = set(new_state_dict.keys())

        # Check for mismatches
        missing = model_keys - checkpoint_keys
        unexpected = checkpoint_keys - model_keys

        if missing:
            print(f"[WARNING] Missing keys in checkpoint: {len(missing)} keys")
            print(f"[WARNING] First 5 missing: {list(missing)[:5]}")

        if unexpected:
            print(f"[WARNING] Unexpected keys in checkpoint: {len(unexpected)} keys")
            print(f"[WARNING] First 5 unexpected: {list(unexpected)[:5]}")

        # Load with strict=False to ignore minor mismatches
        result = raw_model.load_state_dict(new_state_dict, strict=False)

        if result.missing_keys or result.unexpected_keys:
            print(f"[WARNING] Load completed with mismatches (missing: {len(result.missing_keys)}, unexpected: {len(result.unexpected_keys)})")
            if len(result.missing_keys) > 0 and len(result.missing_keys) < 10:
                print(f"[WARNING] Missing keys: {result.missing_keys}")
            if len(result.unexpected_keys) > 0 and len(result.unexpected_keys) < 10:
                print(f"[WARNING] Unexpected keys: {result.unexpected_keys}")
        else:
            print("[INFO] ‚úì Model state dict loaded successfully (perfect match)")

        # Load optimizer and scheduler if available
        start_epoch = 0
        start_step = 0

        if isinstance(checkpoint, dict):
            if optimizer is not None and 'optimizer_state_dict' in checkpoint:
                try:
                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    print("[INFO] ‚úì Optimizer state loaded")
                except Exception as e:
                    print(f"[WARNING] Could not load optimizer state: {e}")

            if scheduler is not None and 'scheduler_state_dict' in checkpoint:
                try:
                    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                    print("[INFO] ‚úì Scheduler state loaded")
                except Exception as e:
                    print(f"[WARNING] Could not load scheduler state: {e}")

            start_epoch = checkpoint.get('epoch', 0)
            if start_epoch > 0:
                start_epoch += 1  # Resume from next epoch
            start_step = checkpoint.get('step', 0)

            print(f"[INFO] ‚úì Checkpoint loaded successfully (resuming from epoch {start_epoch})")
        else:
            print(f"[INFO] ‚úì Model state dict loaded (starting from epoch 0)")

        return start_epoch, start_step

    except Exception as e:
        print(f"[ERROR] ‚úó Failed to load checkpoint: {e}")
        traceback.print_exc()
        print("[WARNING] Starting training from scratch")
        return 0, 0

"""## main ‚Äî Full Training Orchestration Pipeline

The **`main()`** function is the **entry point** for end-to-end model training of **H64LM** ‚Äî  
a custom causal language model supporting local datasets, local tokenizers, multi-GPU setups, and checkpoint resumption.

This function manages the **entire workflow**:  
dataset loading ‚Üí tokenizer setup ‚Üí model creation ‚Üí optimizer/scheduler configuration ‚Üí checkpoint resumption ‚Üí full training ‚Üí visualization.

---

### High-Level Overview

| Stage | Purpose |
|--------|----------|
| 1Ô∏è‚É£ | Initialize config, logger, and random seeds |
| 2Ô∏è‚É£ | Load dataset (local parquet or Hugging Face fallback) |
| 3Ô∏è‚É£ | Load and configure pre-trained tokenizer (local only) |
| 4Ô∏è‚É£ | Build model and configure multi-GPU setup |
| 5Ô∏è‚É£ | Prepare datasets, dataloaders, and optimizer/scheduler |
| 6Ô∏è‚É£ | Optionally resume from a checkpoint |
| 7Ô∏è‚É£ | Launch full training loop with progress logging |
| 8Ô∏è‚É£ | Plot training loss and perplexity curves |

---

### Step-by-Step Breakdown

#### **1. Configuration & Environment Setup**
- Instantiates:
  - `H64LMConfig()` ‚Äì architecture and model-level hyperparameters  
  - `H64LMTrainingConfig()` ‚Äì optimizer, scheduling, and training runtime configs
- Sets deterministic seeds for `torch`, `numpy`, and CUDA for **reproducibility**.
- Detects available GPUs and configures the active device:
  ```python
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""

def main():

    """Main training function with pre-trained tokenizer support."""

    # Configuration
    config = H64LMConfig()
    training_config = H64LMTrainingConfig()

    logger.info(">>> Starting H64LM Training")

    # Set random seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
        torch.backends.cudnn.benchmark = True



    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f">>> Using device: {device}")
    logger.info(f">>> Number of GPUs: {torch.cuda.device_count()}")

    # Load dataset
    logger.info(f">>> Loading dataset: {config.dataset} /{config.dataset_dir}")

    # Prefer local parquet, else fall back to Hugging Face WikiText-103
    data_dir = config.dataset_dir

    try:
        if os.path.exists(data_dir):
            logger.info(">>> Loading local WikiText-103 parquet/text dataset")
            # Try parquet first, fall back to text if not found
            try:
                dataset = load_dataset("parquet", data_dir=data_dir)
            except Exception:
                dataset = load_dataset("text", data_files={
                    "train": os.path.join(data_dir, "wiki.train.raw"),
                    "validation": os.path.join(data_dir, "wiki.valid.raw"),
                    "test": os.path.join(data_dir, "wiki.test.raw"),
                })
        else:
            raise FileNotFoundError("Local dataset not found")

    except Exception as e:
        logger.error(f"Failed to load local dataset: {e}")
        logger.info(">>> Downloading Hugging Face WikiText-103 dataset instead")
        dataset = load_dataset("wikitext", "wikitext-103-raw-v1")
        logger.info(">>> Successfully loaded WikiText-103 from Hugging Face")

    logger.info(dataset)



    # ============================================================================
    # MODIFIED: Use ONLY the local Mistral tokenizer (no internet required)
    # ============================================================================
    logger.info(">>> Loading pre-trained tokenizer")

    # Always use the local Mistral tokenizer in Kaggle
    tokenizer_name = config.use_pretrained_tokenizer
    logger.info(f">>> Using tokenizer: {tokenizer_name}")

    # Load tokenizer (force local_files_only)
    tokenizer = AutoTokenizer.from_pretrained(
        tokenizer_name,
        local_files_only=True,  # Always load from local files (no internet)
        trust_remote_code=True
    )

    # Set special tokens if not present
    tokenizer.padding_side = 'right'

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        logger.info(f">>> Set pad_token to eos_token: {tokenizer.eos_token}")

    if tokenizer.bos_token is None:
        tokenizer.bos_token = tokenizer.eos_token
        logger.info(f">>> Set bos_token to eos_token: {tokenizer.eos_token}")

    logger.info(f">>> Tokenizer loaded successfully")
    logger.info(f">>> Vocab size: {tokenizer.vocab_size}")
    logger.info(f">>> PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
    logger.info(f">>> EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
    logger.info(f">>> BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})")



    # ============================================================================


    # Update config with tokenizer info
    config.vocab_size = tokenizer.vocab_size
    config.pad_token_id = tokenizer.pad_token_id
    config.eos_token_id = tokenizer.eos_token_id
    config.bos_token_id = tokenizer.bos_token_id if tokenizer.bos_token_id else tokenizer.eos_token_id
    logger.info(f">>> Model vocab size = {config.vocab_size}")

    # Initialize model
    logger.info(">>> Initializing model")
    model = H64LMForCausalLM(config, tokenizer)

    # Multi-GPU support
    if config.use_ddp and torch.cuda.device_count() > 1:
        logger.info(f">>> Using DistributedDataParallel with {torch.cuda.device_count()} GPUs")
        logger.warning("DDP requires running with: python -m torch.distributed.launch --nproc_per_node=N train.py")
        model = torch.nn.DataParallel(model)
    elif torch.cuda.device_count() > 1:
        logger.info(f">>> Using DataParallel with {torch.cuda.device_count()} GPUs")
        model = torch.nn.DataParallel(model)

    model = model.to(device)

    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f">>> Total parameters: {total_params:,}")
    logger.info(f">>> Trainable parameters: {trainable_params:,}")

    # Prepare training dataset
    logger.info(">>> Preparing training and validation datasets")

    all_samples = []
    max_samples = config.max_samples

    for i, ex in enumerate(dataset['train']):
        if i >= max_samples:
            break

        if isinstance(ex, dict):
            text = str(ex.get("text", "")).strip()
        else:
            text = str(ex).strip()

        if len(text) > 20:
            all_samples.append({"text": text})

    if not all_samples:
        logger.error(">>> ERROR: No valid data found")
        return None

    # Shuffle and split
    np.random.shuffle(all_samples)
    split_idx = int(0.9 * len(all_samples))
    train_list = all_samples[:split_idx]
    val_list = all_samples[split_idx:]

    logger.info(f">>> Training samples: {len(train_list)}")
    logger.info(f">>> Validation samples: {len(val_list)}")




    # Setup optimizer and scheduler (needed before loading checkpoint)
    train_dataset_temp = Dataset.from_list(train_list)
    collate_fn_temp = create_collate_fn(tokenizer, config)
    train_loader_temp = DataLoader(
        train_dataset_temp,
        batch_size=config.batch_size,
        collate_fn=collate_fn_temp,
        shuffle=True,
        drop_last=True
    )

    total_steps = (len(train_loader_temp) * config.num_epochs ) // config.grad_accum_steps
    base_lr = 3e-4
    no_decay = ["bias", "LayerNorm.weight", "layernorm.weight", "norm.weight"]

    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in model.named_parameters()
                      if not any(nd in n for nd in no_decay) and p.requires_grad],
            "weight_decay": 0.01,
        },
        {
            "params": [p for n, p in model.named_parameters()
                      if any(nd in n for nd in no_decay) and p.requires_grad],
            "weight_decay": 0.0,
        },
    ]

    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=base_lr, betas=(0.9, 0.95), eps=1e-8)
    warmup_steps = max(1, total_steps // 10)
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)

    # RESUME FROM CHECKPOINT
    start_epoch = 0

    if config.resume_from and os.path.exists(config.resume_from):
        print(f"[INFO] >>> Attempting to resume from: {config.resume_from}")
        start_epoch, _ = load_checkpoint_for_resume(
            model, optimizer, scheduler, config.resume_from , device
        )

        # If resuming, adjust remaining epochs
        if start_epoch > 0:
            config.num_epochs = max(config.num_epochs, start_epoch + 10)  # Train 10 more epochs
            print(f"[INFO] >>> Will train until epoch {config.num_epochs}")



    # Train the model
    logger.info(">>> Starting training loop")
    history = train_model(
        model=model,
        config=config,
        training_config=training_config,
        train_dataset=train_list,
        val_dataset=val_list,
        tokenizer=tokenizer,
        num_epochs=config.num_epochs,
        batch_size=config.batch_size,
        grad_accum_steps=config.grad_accum_steps,
        log_interval=config.log_interval,
        save_interval=config.save_interval,
        checkpoint_dir=config.checkpoint_dir
    )
    logger.info(">>> Training finished!")

    # Plot training curves if matplotlib available
    try:

        fig, axes = plt.subplots(1, 2, figsize=(12, 4))

        # Loss curves
        axes[0].plot(history['train_loss'], label='Train Loss', marker='o')
        axes[0].plot(history['val_loss'], label='Val Loss', marker='s')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].set_title('Training and Validation Loss')
        axes[0].legend()
        axes[0].grid(True)

        # Perplexity curve
        axes[1].plot(history['val_perplexity'], label='Val Perplexity', marker='s', color='orange')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Perplexity')
        axes[1].set_title('Validation Perplexity')
        axes[1].legend()
        axes[1].grid(True)

        plt.tight_layout()
        plt.savefig('checkpoints/training_curves.png', dpi=150)
        logger.info(">>> Training curves saved to checkpoints/training_curves.png")
        plt.show()
    except ImportError:
        logger.warning(">>> matplotlib not available, skipping training curves")

    return model, tokenizer, device, history

"""## test_generation ‚Äî Post-Training Text Generation Evaluation

The **`test_generation()`** function performs a **qualitative test** of a trained language model by prompting it to generate text continuations.  
It‚Äôs typically run **after training** or **after checkpoint restoration** to verify inference quality and model coherence.

---

### Purpose

| Goal | Description |
|------|-------------|
| Sanity Check | Confirm that the trained model can successfully load and generate text |
| Qualitative Evaluation | Visually inspect fluency, topical relevance, and creativity |
| Decoding Demo | Showcase autoregressive generation capabilities of the H64LM model |

---

### Function Signature

```python
def test_generation(
    model,
    tokenizer,
    device,
    prompt="The history of deep learning",
    max_new_tokens=100
)

"""

def test_generation(model, tokenizer, device, prompt="The history of deep learning", max_new_tokens=100):
    """Test the trained model with text generation."""
    print(f"[INFO] >>> Testing generation")
    print(f"[INFO] >>> Prompt: {prompt}")

    # Unwrap DataParallel if needed
    raw_model = model.module if hasattr(model, "module") else model
    raw_model.eval()

    with torch.no_grad():
        # Tokenize prompt
        inputs = tokenizer(prompt, return_tensors="pt")
        input_ids = inputs["input_ids"].to(device)

        # Generate using the model's method
        output_ids = raw_model.generate(
            input_ids=input_ids,
            max_new_tokens=max_new_tokens,
            temperature=0.8,
            top_p=0.9,
            top_k=50,
            do_sample=True
        )

    # Decode
    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    print(f"[INFO] >>> Generated text:\n{generated_text}\n")

    return generated_text


if __name__ == "__main__":


    result = main()

    # Test generation if training succeeded
    if result is not None:
        model, tokenizer, device, history = result

        logger.info("\n>>> Testing generation with trained model")
        test_generation(model, tokenizer, device, "The history of Computer")
        test_generation(model, tokenizer, device, "Define Human")
        test_generation(model, tokenizer, device, "IS Deep learning a Machine learning (ML)")
        test_generation(model, tokenizer, device, "In the future, humans and machines")
        test_generation(model, tokenizer, device, "Deep learning is a subfield of machine learning that")
        test_generation(model, tokenizer, device, "Once upon a time,")

        # Print final metrics
        logger.info("\n>>> Final Training Metrics:")
        logger.info(f"Best Validation Loss: {min(history['val_loss']):.4f}")
        logger.info(f"Best Validation Perplexity: {min(history['val_perplexity']):.2f}")
        logger.info(f"Final Train Loss: {history['train_loss'][-1]:.4f}")
        logger.info(f"Final Val Loss: {history['val_loss'][-1]:.4f}")
        logger.info(f"Average Throughput: {sum(history['tokens_per_second'])/len(history['tokens_per_second']):.0f} tokens/sec")